\documentclass[conference,compsoc]{IEEEtran}
% \documentclass{sig-alternate-05-2015}                

% \usepackage[dvipdfmx]{graphicx} \usepackage{amssymb} \usepackage{multirow} \usepackage{threeparttable} \usepackage{array} \usepackage{color} \usepackage{nidanfloat}
\usepackage{graphicx} \usepackage{amssymb} \usepackage{multirow} \usepackage{threeparttable} \usepackage{array} \usepackage{color} \usepackage{nidanfloat}\usepackage{setspace}

% \newcommand{\comment}[1]{\colorbox{green}{#1}}
\newcommand{\comment}[1]{}

% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\author{
  \IEEEauthorblockN{Yuya Maruyama}
  \IEEEauthorblockA{Graduate School of Engineering Science\\Osaka University}
  \and
  \IEEEauthorblockN{Shinpei Kato}
  \IEEEauthorblockA{Graduate School of\\Information Science and Technology\\The University of Tokyo}
  \and
  \IEEEauthorblockN{Takuya Azumi}
  \IEEEauthorblockA{Graduate School of Engineering Science\\Osaka University}
}

% \numberofauthors{2}
% \author{
% % 1st. author
% \alignauthor Yuya Maruyama\\
% \affaddr{Graduate School of Engineering Science}\\
% \affaddr{Osaka University}\\
%        % \affaddr{Institute for Clarity in Documentation}\\
%        % \affaddr{1932 Wallamaloo Lane}\\
%        % \affaddr{Wallamaloo, New Zealand}\\
%        % \email{trovato@corporation.com}
% % 2nd. author
% % \alignauthor Shinpei Kato\\
% % \affaddr{Graduate School of Information Science and Technology}\\
% % \affaddr{The University of Tokyo}\\
%        % \affaddr{P.O. Box 1212}\\
%        % \affaddr{Dublin, Ohio 43017-6221}\\
%        % \email{webmaster@marysville-ohio.com}
% % 3rd. author
% \alignauthor Takuya Azumi\\
% \affaddr{Graduate School of Engineering Science}\\
% \affaddr{Osaka University}\\
% }


% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


%% % DOI
%% \doi{10.475/123_4}

%% % ISBN
%% \isbn{123-4567-24-567/08/06}

%% %Conference
%% \conferenceinfo{EMSOFT '16}{October 2--7, 2016, Pittsburgh, PA, USA}

%% \acmPrice{\$15.00}

%
% --- Author Metadata here ---
% \conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{\comment{1-2, 3-2}Exploring Scalable Data Allocation and Parallel Computing \\ on NoC-based Embedded Many Cores}
\begin{document}

%% ACM copyright information
% \CopyrightYear{2016}
% \setcopyright{othergov}
% \conferenceinfo{EMSOFT'16,}{October 01-07, 2016, Pittsburgh, PA, USA}
% \isbn{978-1-4503-4485-2/16/10}\acmPrice{\$15.00}
% \doi{http://dx.doi.org/10.1145/2968478.2968502}

\maketitle

\setcounter{topnumber}{5}%    ページ上部の図表は 5 個まで
\def\topfraction{1.00}%       ページの上 1.00 まで図表で占めて可
\setcounter{bottomnumber}{5}% ページ下部の図表は 5 個まで
\def\bottomfraction{1.00}%    ページの下 1.00 まで図表で占めて可
\setcounter{totalnumber}{10}% ページあたりの図表は 10 個まで
\def\textfraction{0.00}%      ページうち本文が占める割合の下限

% \setlength\textfloatsep{18pt}%  between figure and text
% \setlength\abovecaptionskip{4pt}% between figure and caption
% \setlength\floatsep{18pt}% between figures

\comment{TBD:1-1, 1-14, 2-2, 3-5}

\begin{abstract}
In embedded systems, high processing requirements and low power consumption need heterogeneous computing platforms.
% The evolution of a next generation computing platform oriented toward multi/many cores is inevitable.
Considering embedded requirements, applications need to be designed based on scalable data allocation and parallel computing with non-uniform memory access (NUMA) many cores.
In this paper, we use one of the embedded commercial off-the-shelf (COTS) multi/many-core components, the Massively Parallel Processor Arrays (MPPA) 256 developed by Kalray, and conduct evaluations of data transfer and parallelization of practical application.
We investigate currently achievable data transfer latencies between distributed memories on networks-on-chip (NoC), memory access characteristics, and parallelization potential with many cores.
Subsequently, we run a practical application, the core of the autonomous driving system, on many-core processors and acceleration by parallelization indicates practicality of many cores.
By highlighting many-core computing capabilities, we explore the scalable data allocation and parallel computing on NoC-based embedded many cores.
% The evolution of a next generation computing platform oriented toward multi/many cores is inevitable because the demand for high-performance computation with reasonable power consumption even in embedded real-time systems is increasing.
% The Multi-Purpose Processing Array 256 developed by Kalray is a many-core processor platform that includes 256 general-purpose cores with high energy efficiency.
% The clusters of the cores share a local memory, and a DMA-capable network-on-chip (NoC) connects the clusters.
% In this paper, we investigate and characterize currently achievable data transfer methods of cutting-edge many-core technologies based on NoC.
% The cores communicate with each other by NoC parallelly to manage numerous tasks and optimize the potential of this architecture.
% Previous studies have not quantitatively examined the transport latencies nor have they clarified the potential and constraints for low-latency many-core computing.
% To address this problem, we illustrate evaluations of end-to-end latencies and the calculation potential of many-core computing with parallel data transfer.
% Our experimental results will allow system designers to choose appropriate data transfer methods based on the requirements of their latency-sensitive many-core applications as opposed to just intuitive expectations.
% By highlighting many-core computing capabilities, we explore the performance characteristics of currently achievable data transfer methods for many-core computing based on NoC.
\end{abstract}

% \keywords{Many-core; Multicore; Network-on-Chip (NoC); Parallelization; Embedded software}

\vspace{-3mm}
\section{Introduction}
\vspace{-3mm}
\comment{1-4, 1-5, 1-6, 2-1, 2-4}
The evolution of a next generation computing platform oriented toward multi/many cores is unavoidable to satisfy the demand for increasing computation in conjunction with reasonable power consumption in several domains such as automobiles.
In embedded systems, this trend is also adapted because of high processing requirements.
For example, an autonomous driving system involves various applications and are sometimes characterized by demands for high-performance computing.
Considering requirements of high processing, predictability, and energy efficiency for autonomous driving systems, its system needs a heterogeneous computing system such as multi/many cores and GPUs.
\comment{1-3} In embedded systems, Multi/many-core architecture is an important trend as it integrates cores to realize high-performance and general-purpose computing with low power consumption \cite{becker2016contention}, \cite{perret2016mapping}.
% Extant studies have examined several applications of multi/many-core platforms \cite{becker2016contention}, \cite{saidi2015shift}, \cite{perret2016temporal}, \cite{perret2016mapping}, \cite{becker2014mapping}.
% Recently, the computational ability of single core processors has reached its limit \comment{1-3} and thus the applicability of Moore's law \cite{moore2006cramming} is unclear.
% Multi/many core architecture is an important trend in \comment{1-3} recent years as it integrates cores to realize high-performance and general-purpose computing with low power consumption.
% Hence, high energy efficiency is a superior feature of multi/many core platforms.

% As detailed in the preceding paragraph, the demand for increasing computation in conjunction with reasonable power consumption drives the need for multi/many core architecture in several domains.
% Real-time embedded systems are an example in which multi/many core platforms are adopted since they face increasing processing requirements.
% Extant studies have examined numerous applications of multi/many cores platforms \cite{becker2016contention}, \cite{saidi2015shift}, \cite{perret2016temporal}, \cite{perret2016mapping}, \cite{becker2014mapping}.

% For example, automotive systems involve various applications and are sometimes characterized by demands for high-performance computing.
% Automotive applications are responsible for several control systems such as the powertrain, chassis, steering wheel, driver assistance and user interface.
% Advanced driver assistance systems are characterized by increasing complexity and computational requirements and necessitate intelligence such as an autonomous driving system.
% Moreover, they also require significant energy efficiency and cost reduction.
% Although modern automotive systems are composed of several Electronic Control Unit (ECU) managing subsystems, there is a shift from numerous scattered ECUs to hierarchical multi/many core Domain Controllers (DC).
% Evidently, a hybrid scenario is also applicable. 
% \comment{1-4}
% Specifically, DCs combine performance with low power consumption to realize high parallel processing and power efficiency.
% \comment{1-5}
% % Furthermore, flexible scalability of a many-core platform facilitates development efficiency.

% A related example involves avionics systems that also include various applications and require cost efficiency.
% \comment{1-6}
% % Multi/many core platforms can be applied in the avionics domain due to their highly integrated characteristics and increasing processing demands.
% Low power consumption of cores leads to lower cooling requirements, and this is a critical issue for avionics.
% Multi/many core platforms reduce energy consumption as well as the costs and weight.
% Weight reduction is the most important factor for avionics, and integration with a multi/many core platform reduces the number of processing boards and cooling units.
% Thus, an integrated platform results in space, weight and cost reductions.

% Multi/many core platforms are fabricated and released as commercial off-the-shelf (COTS) multicore components.
% Increasing research attention has focused on multi/many core platforms.
% The Multi-Purpose Processing Array (MPPA) 256 developed by Kalray, \cite{de2014time}, Single-chip Cloud Computer (SCC) developed by Intel \cite{baron2010single}, and Tile64 developed by Tilera \cite{bell2008tile64} include the clustering of many-core architectures in which cores are \comment{1-7} allocated closely.
% The clusters of cores are capable of performing separate independent applications with respect to the desired power envelope of embedded applications.
% Kalray's MPPA-256 packs 256 general-purpose cores.
% This significantly exceeds the number of cores in other COTS, and it targets embedded systems, high-performance computing (HPC), image processing, and networking due to the high-performance per watt.
% Intel's Xeon Phi \cite{chrysos2014intel}, \cite{chrysos2012intel} is an HPC accelerator.
% Xeon Phi and SCC are based on x86 architecture, and its targeted use are data centers and workstations.

Despite the emergence of the need for multi/many-core platforms, several difficulties persist in the adaptation of these platforms to embedded systems \cite{becker2016contention}, \cite{saidi2015shift}.
These difficulties are caused by strict requirements of embedded systems and the hardware architecture shares resources (e.g. memory subsystems and I/O devices) by many cores.
Parallelized processes share memories, which leads to the frequent occurrence of conflicts.
% Shared resources also disturb predictable timing behavior and software analysis.
% Cache coherency is also a critical problem owing to their numerous cores.
% Although it is possible to connect a large memory and all the cores with wide-bandwidth buses, it will lose its scaleability due to bus competition and it will require a large power consumption.
MPPA-256 \cite{de2014time} is one of the commercial off-the-shelf (COTS) multi/many-core components targeting embedded system
It is developed by Kalray and adopt non-uniform memory access (NUMA) using network-on-chip (NoC), and realize numerous cores and low power consumption.
However, its data transfer between distributed memories with NoC, parallelization potential, and memory access characteristics are not fully unrvealed for application developers.
This work explores above issues with quantitative evaluations and a practical application.
% The timing requirement of real-time embedded systems continues to warrant solutions.
% The impact of integrating applications in multi/many-core platforms is not completely understood to date.
% This is a critical issue because embedded systems have requirements for reliable and predictable behavior.

% Considering embedded requirements, multi/many cores need NUMA because of scalability of the number of cores and reasonable power consumption.
% Scalable data allocation enhances parallelized high-performance and general-purpose computing with low power consumption.
% MPPA-256 is one of the commercial off-the-shelf (COTS) multi/many-core components targeting embedded system and adopts distributed memory architecture with NoC. (We discusses comparison to other platforms in Section \ref{sec:related_work}.)
% However, its data transfer between distributed memories with NoC, parallelization potential, and memory access characteristics are not fully unrvealed for application developers.
% This work explores above issues with quantitative evaluations and a practical application.

\textbf{Contributions:}
\comment{1-1, 3-3: To be clarified}
This work focuses on examining embedded many-core computing based on NoCs, such as MPPA-256.
We conduct evaluations of data transfer methods on NoCs to clarify latency characteristics of data transfer and an influence of data allocation.
Subsequently, we parallelize localization algorithm which is the core of the autonomous driving system.
We reveal the advantages and disadvantages of embedded many-core computing based on NoC in a quantitative way by leading the following contributions:

\vspace{-3mm}
\begin{itemize}
\setlength{\leftskip}{-5mm}
\item The evaluations of the data transfer on DMA-capable NoC quantitatively characterize the end-to-end latencies, which depend on the routing and DMA configurations.
\item The scalability of parallelization in many-core processors based on NoC is observed in a real complex application as a part of an autonomous driving system.
% \item The evaluations of the parallel processing examine the characteristics of the memory access speed which varies according to where data is allocated and what accesses the memory.
\end{itemize}
\vspace{-3mm}

% To the best of our knowledge, this is the first work that examines data transfer and data allocation matters for many-core computing beyond an intuitive expectation to allow system designers to choose appropriate data transfer methods.
% Additionally, the speed-up result of autonomous driving application indicates a practical potential for NoC-based embedded many-core computing.
\comment{1-8, 1-9}
% It is expected that the findings of this study will potentially be applicable to several types of many-core architectures as opposed to just MPPA-256.
% Therefore, the contributions of this study can be used in low-latency many-core computing.

% \textbf{Organization:}
% The remainder of this work is organized as follows.
% First, the system model considered in this work is discussed in Section \ref{sec:system_model} in which the hardware model, namely Kalray MPPA-256 Bostan, and the system model are presented.
% \comment{1-10}
% Second, Section \ref{sec:evaluations} explains evaluation setup and approach, and illustrates experimental evaluations.
% Subsequently, Section \ref{sec:related_work} examines related work that focus on multi/many-core systems.
% Finally, Section \ref{sec:conclusion} presents the conclusions and directions for future research.


\vspace{-2mm}
\section{System Model}
\label{sec:system_model}
\vspace{-3mm}
This section presents the system model used throughout the work.
The many-core model of Kalray MPPA-256 Bostan is considered.
% First, a hardware model is introduced in Section \ref{sec:hardware_model}, and this is followed by a software model in Section \ref{sec:software_model}.

\vspace{-3mm}
\subsection{Hardware Model}
\label{sec:hardware_model}
\vspace{-3mm}
The MPPA-256 processor is based on an array of compute clusters (CCs) and I/O subsystems (IOSs) that are connected to nodes of Network-on-Chip (NoC) with a toroidal 2D topology 
(as shown in Figures \ref{fig:mppa_architecture} and \ref{fig:noc_map}).
The MPPA MANYCORE chip integrates 16 compute clusters and 4 IOSs on NoC.
% The architecture of Kalray MPPA-256 is presented in this section.

\vspace{-2mm}
\subsubsection{I/O Subsystems (IOS)}
\label{sec:ios}
MPPA-256 has the following four I/O subsystems (IOSs): North, South, East, and West IOSs.
The North and South IOSs are connected to a DDR interface and an eight-lane PCIe controller.
The East and West IOSs are connected to a quad 10Gb/s Ethernet controller.
Two pairs of IOSs organize two I/O clusters (IOCs) as shown in Figure \ref{fig:mppa_architecture}.
Each IOS comprises of quad IO cores and a NoC interface.

\textbf{IO cores}: IO cores are connected to a 16-banked parallel shared memory with a total capacity (IO SMEM) of 2 MB, as shown in Figure \ref{fig:mppa_architecture},
The four IO cores have their own instruction cache 8-way associative corresponding to 32 ($8 \times 4$) KB and share a data cache 8-way with 128 KB and external DDR access.
The sharing of the data cache of 128 KB allows coherency between the IO cores.
Additionally, IO cores operate controllers for the PCIe, Ethernet, \comment{1-11} and other I/O devices.
% They operate the local peripherals, including the NoC interfaces with DMA.
% It is also possible to conduct an application run on the IO cores.

\textbf{NoC Interface}: The NoC interface contains four DMA engines (DMA1-4) and four NoC routers as shown in Figure \ref{fig:mppa_architecture}, and the IOS DMA engine manages transfers between the IO SMEM, the IOS DDR, and the IOS peripherals (e.g., PCIe interface and Ethernet controllers).
The DMA engine transfers data between routers on NoC through NoC routers and it has the following three NoC interfaces: a receive (Rx) interface, a transmit (Tx) interface, and a micro core (UC).
A UC is a fine-grained multi-threaded engine that can be programed to set threads sending data with a Tx interface
A UC can extract data from memory by using a programed pattern and send the data on the NoC.
After it is initiated, this continues in an autonomous fashion without using a Processing Element (PE) and an IO core.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{../figure/mppa_architecture.eps}
  \vspace{-7mm}
  \caption{\label{fig:mppa_architecture}
    An overview of the architecture of the Kalray MPPA-256 Bostan.}
  \vspace{-5mm}
\end{figure}

\vspace{-2mm}
\subsubsection{Compute Clusters (CC)}
\label{sec:cc}
In MPPA-256, the 16 inner nodes of the NoC correspond to the CCs.
Figure \ref{fig:mppa_architecture} illustrates the architecture of each CC.

\textbf{Processing Elements and an RM}:
\comment{2-3}
In a CC, 16 processing elements (PEs) and an RM share 2 MB cluster local memory (SMEM).
% The capacity of each SMEM bank is 128 KB.
The PEs are mainly used by users for parallel processing.
Developers spawn computing threads on PEs.
The PEs and an RM in CC correspond to the Kalray-1 cores, which implement a 32-bit 5-issue Very Long Instruction Word architecture with 600 or 800 MHz.
Each core is fitted with its own instruction and data caches.
% Each cache is a 2-way associative with a capacity of 8 KB.
% Thus, 17 k1-cores (a PE or the RM) share a multi-banked 2 MB SMEM.

% \textbf{A Debug Support Unit and a NoC Interface}:
% In addition to PEs and an RM, bus masters on SMEM correspond to a Debug Support Unit and a DMA engine in a NoC interface.
% A DMA engine and a NoC router are laid out in a NoC interface.
% The CC DMA engine also has the following three interfaces: an Rx, a Tx, and a UC. 
% % It is instantiated in every cluster and connected to the SMEM.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=1.0\linewidth]{../figure/cc_architecture.eps}
%   \caption{\label{fig:cc_architecture}
%     Compute Cluster architecture.}
% \end{figure}

\vspace{-2mm}
\subsubsection{Network-on-Chip (NoC)}
\label{sec:noc}
The 16 CCs and the 4 IOSs are connected by a NoC as shown in Figure \ref{fig:noc_map}.
Furthermore, a NoC is constructed \comment{1-12} as the bus network and has routers on each node.

\textbf{Bus Network}:
Bus network connects nodes (CCs and IOSs) with torus topology \cite{dally2001route}
which \comment{1-13} involves a low average number of hops when compared to mesh topology.
The network is actually composed of the following two parallel NoCs with bi-directional links (denoted by red lines in Figure \ref{fig:noc_map}):
the data NoC (D-NoC) that is optimized for bulk data transfers and the control NoC (C-NoC) that is optimized for small messages at low latency.
% The NoC is implemented with wormhole switching and source routing.
Data is packaged in variable length packets which are broken into small pieces called flits (flow control digits).
% The NoC traffic is segmented into packets, and each packet includes 1 to 4 header flits and 0 to 62 payload data flits.

\textbf{NoC routers}:
A node per compute cluster and four nodes per I/O subsystem hold the following two routers of its own: a D-NoC router and a C-NoC router.
Each RM or IO core on NoC node is associated with the fore-mentioned two NoC routers.
Furthermore, DMA engines in a NoC interface on the CC/IOS send and receive flits through the D-NoC routers with the Rx interface, the Tx interface, and the UC.
% A mailbox component corresponds to the virtual interface for the C-NoC and enables one-to-one, N-to-one, or one-to-N low-latency synchronizations.
% The NoC routers shown in Figure \ref{fig:mppa_architecture} illustrate nodes as R1-16, R128-131, R160-163, R224-227, and R192-195 in Figure \ref{fig:noc_map}.
The NoC routers are illustrated as nodes in Figure \ref{fig:noc_map}.
% For purposes of simplicity, D-NoC/C-NoC routers are illustrated with a NoC router.
% In both D-NoC and C-NoC, each network node (CC or IOS)  includes the following 5-link NoC routers:
% four duplexed links for north/east/west and south neighbors and a duplexed link for local address space attached to the NoC router.
% The NoC routers include FIFOs queuing flits for each direction.
% The data links are four bytes wide in each direction and operate at the CPU clock rate of 600 MHz or 800 MHz, and therefore each tile can transmit/receive a total of 2.4 GB/s or 3.2 GB/s, which is spread across the four directions (i.e., north, south, east, and west).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.65\linewidth]{../figure/noc_map.eps}
  \vspace{-4mm}
  \caption{\label{fig:noc_map}
    NoC connections (both D-NoC and C-NoC).}
  \vspace{-5mm}
\end{figure}

\vspace{-3mm}
\subsection{Software Model}
\label{sec:software_model}
\vspace{-3mm}
% Figure \ref{fig:software_stack} shows the software stack used for Kalray MPPA-256 in the present work.
% The Kalray system is an extensible and scalable array of computing cores and memory.
The software stack used for Kalray MPPA-256 is composed of hardware abstraction layer, low-level layer, OS, and user application.
With respect to scalable computing array of the system, it is possible to map several programming models or runtimes such Linux, real-time operating system, POSIX API, OpenCL, and OpenMP.
% Each layer is described in detail.

In the hardware abstraction layer, an abstraction package abstracts hardware of a CC, IOS, and NoC.
\comment{1-15} % The abstraction package serves as a system that does not provide any services.
The hardware abstraction is responsible for partitioning hardware resources and controlling access to the resources from the user-space operating system libraries.
\comment{1-16} 
% Additionally, the abstraction package retrieves resources allocated to a partition at any time.
It sets-up and controls inter-partition communications as a virtual machine abstraction layer.
The hardware abstraction runs on the dedicated RM core.
All the services are commonly provided by a operating system (e.g., virtual memory and schedule) that must be provided by user-space libraries.
% Consequently, each runtime or operating system implements its own services that are optimized to specific needs.
% This is because each programming model or runtime involves different requirements.
A minimal kernel avoids wastage of resources and mismatched needs.

In a low-level library layer, the Kalray system also provides libraries for handling NoC.
Additionally, NoC features such as routing and quality of service are set by the programmer.
The Libnoc allows direct access to memory mapped registers for their configurations and uses.
It is designed to cause a minimum amount of CPU overhead.
% It also serves as a minimal abstraction for resource allocation.
% Librouting offers a minimal set of functions that can be used to route data between any clusters of the MPPA including unicast (one target) modes or multicast (multiple targets) modes.
Librouting offers a minimal set of functions that can be used to route data between any clusters of the MPPA.
Routing on the network is statically conducted with its own policy.

% \comment{1-17}
% Routing on the torus network is statically conducted with its own policy.
% The Libpower enables spawning and waiting for the end of execution of a remote cluster.

% Various operating systems support the abstraction package in the OS layer.
% The following Real-Time Operating System (RTOS) is introduced:

% \textbf{RTEMS}: RTEMS (Real-Time Executive for Multiprocessor Systems) is a full featured RTOS prepared for embedded platforms.
% It supports several APIs and standards, and most notably supports the POSIX API.
% % The system provides a rich set of features, and an RTEMS application mostly is a regular C or C++ program that uses the POSIX API.
% \comment{1-18}
% RTEMS runs on the IOC except for the CC.

% \textbf{NodeOS}: On CC, the MPPA cluster operating system utilizes a runtime called NodeOS.
% The OS addresses the need for a multicore OS to conform to the maximum possible extent to the standard POSIX API.
% The NodeOS enables a user code by using POSIX API to run on PEs on CC.
% % First, NodeOS runtime starts on PE0 prior to calling the user main function.
% % Subsequently, \texttt{pthread} is called on other PEs.

% \textbf{eMCOS}: On both CC and IOS, eMCOS provides minimal programming interfaces and libraries.
% Specifically, eMCOS is a real-time embedded operating system developed by eSOL (a Japanese supplier for RTOS), and eMCOS is the world's first commercially available many-core RTOS for use in embedded systems.
% The OS implements a distributed micro-kernel architecture.
% % This compact micro-kernel is equipped with only minimal functions.
% It enables applications to operate priority based message passing, local thread scheduling, and thread management on IOS as well as CC.
% % RTMES and NodeOS are provided by Kalray and eMCOS is released by eSOL.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.8\linewidth]{../figure/softwarestack.eps}
%   \vspace{-3mm}
%   \caption{\label{fig:software_stack}
%     The software stack of Kalray MPPA-256.}
%   \vspace{-5mm}
% \end{figure}


\vspace{-2mm}
\section{Evaluations}
\label{sec:evaluations}
\vspace{-3mm}
At First, this section involves examining two types of evaluations, namely a D-NoC data transfer evaluation in which latency characteristics of interfaces and memory type are explored.
Subsequently, we conduct practical autonomous driving application to examine practicality of NUMA many cores.
% Finally, we arrange lessons learned from above evaluations.
The following all evaluations are conducted on real hardware board with eMCOS which is a real-time embedded operating system developed by eSOL (a Japanese supplier for RTOS).

\vspace{-3mm}
\subsection{D-NoC Data Transfer}
\label{sec:dnoc_eval}

\vspace{-3mm}
\subsubsection{Situations and Assumptions}
\label{sec:situations_and_assumptions}

This evaluation involves clarifying end-to-end latency by considering the relation among interfaces (Tx or UC), routing on NoC, and memory type (DDR or SMEM).
This is achieved by preparing four routes as shown in Figure \ref{fig:noc_routes}.
The routes on D-NoC map (Figure \ref{fig:noc_map}) contains various connections between routers, namely a direct link, a cross-link, and a flying link.
With respect to the case of routes from the IOS routers to the CC routers, transmitted data is allocated in DDR or IO SMEM.
The CC only includes SMEM as shown in Figure \ref{fig:mppa_architecture}.
% A low-level library is directly used to transfer data with D-NoC.
The transferred data correspond to 100 B, 1 KB, 10 KB, 100 KB, and 1 MB.
The buffers are sequentially allocated in DDR or SRAM (IO SMEM or CC SMEM).
The capacity of CC SMEM is 2 MB, and thus it is assumed that the appropriate communication buffer size is 1 MB.
Given the assumption, the other memory area corresponds to the application, library, and operating system.
End-to-end latencies are measured 1,000 times in numerous situations as shown in Figures \ref{fig:DDR_tx}, \ref{fig:DDR_uc}, \ref{fig:IO_SMEM_tx}, and boxplots are obtained as depicted in Figures \ref{fig:IO_SMEM_uc} and \ref{fig:tx_uc_log}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{../figure/noc_routes.eps}
  \vspace{-3mm}
  \caption{\label{fig:noc_routes}
    Four D-NoC routes used in the evaluation.}
  \vspace{-5mm}
\end{figure}

% \vspace{-3mm}
% \subsection{Data Transfer Framework}
% \label{sec:framework}
% \vspace{-3mm}
\vspace{-3mm}
\subsubsection{Data Transfer Methods}
\label{sec:data_transfer_methods}

\comment{TBD: 1-19: To be fully refleshed, Add setup and approach}
In this section, data transfer methods in MPPA-256 are explained.
For scalability purposes, MPPA-256 accepts a clustered architecture which avoids frequent memory contention between numerous cores.
% For scalability purposes, MPPA-256 accepts clustered architectures in which each cluster contains its own memory.
% 16 cores are packed as a cluster and they share 2 MB memory (SMEM) as shown in Figure \ref{fig:mppa_architecture}.
% This avoids frequent memory contention due to numerous cores and helps in increasing the number of cores.
However, the architecture constraints memory that can be directly accessed by the cores.
In order to communicate with cores outside the cluster, it is necessary to transfer data between clusters through the D-NoC with NoC interfaces.

An Rx interface exists on the receiving side to receive data with DMA.
It is necessary to allocate a D-NoC Rx resource and configure it to wait to receive the data.
A DMA in a NoC interface contains 256 D-NoC Rx resources.
Two interfaces, namely a Tx interface and a UC interface as explained in Sections \ref{sec:ios} and \ref{sec:cc},
are present with respect to the sending side for users to send data between clusters.
The UC is a network processor that is programmed to set threads to send data in DMA.
It executes programed pattern and sends data through the D-NoC without a PE and an RM.
% The UC interface results in higher data transfer throughput compared to the direct activation of the Tx interface by a RM or PE core.
% However, a DMA in a NoC interface contains only 8 D-NoC UC resources.
% Both interfaces use a DMA engine to access memory and copy data.
Irrespective of whether or not a UC interface is used, it is necessary to allocate a D-NoC Tx resource and configure it to send data.
Additionally, it is necessary to allocate and configure D-NoC UC resources if a UC interface is used.

\vspace{-3mm}
\subsubsection{Influences of Routing and Memory Type}
\label{sec:routing_and_memory}

\comment{1-20, 1-21}
Data transfer latencies between IOS and CC are not hardly influenced by routing.
This involved preparing two interfaces (Tx and UC), three routes(direct link, cross-link, and detour route), and two memory locations in which the transferred data is allocated.
As shown in Figures \ref{fig:DDR_tx}, \ref{fig:DDR_uc}, \ref{fig:IO_SMEM_tx}, and \ref{fig:IO_SMEM_uc}, 
end-to-end latency scales exhibit a linear relation with data size, and there are no significant differences between the three routes with respect to data transfer latency.
This result is important in torus topology NoC because the number of minimum steps exceeds those in the mesh topology. 
It is observed that queuing in the NoC routers and hardware distance on the NoC are not dominant factors for latency.
The router latency, the time taken by transmitting and receiving transactions in a RM, exceeds those of other transaction.
Additionally, it is briefly recognized that the speed of UC exceeds that of Tx.
The data is arranged as shown in Figure \ref{fig:tx_uc_log} to facilitate a precise analysis with respect to the interface and memory location.
In Figure \ref{fig:tx_uc_log}, only the cross-link from IOS to CC5 is accepted because routes do not influence latency.
% In order to facilitate intuitive recognition, two kinds of figures are arranged, namely a logarithmic axis and a linear axis.

\begin{figure*}[t]
  \tabcolsep = 0.5mm              % side-margin in column
  \begin{tabular}{cccc}
    \begin{minipage}[t]{0.24\textwidth}
      \includegraphics[width=1.0\linewidth]{../../R/mppa/graph/eps/BoxPlot_DDR_tx.eps}
      \vspace{-7mm}
      \caption{Data transfer with Tx from IO DDR to CC.}
      \label{fig:DDR_tx}
    \end{minipage}
    &
    % \setcounter{figure}{8}
    \begin{minipage}[t]{0.24\textwidth}
      \includegraphics[width=1.0\linewidth]{../../R/mppa/graph/eps/BoxPlot_DDR_uc.eps}
      \vspace{-7mm}
      \caption{Data transfer with UC from IO DDR to CC.}
      \label{fig:DDR_uc}
    \end{minipage}
    &
    % \setcounter{figure}{10}
    \begin{minipage}[t]{0.24\textwidth}
      \includegraphics[width=1.0\linewidth]{../../R/mppa/graph/eps/BoxPlot_IO_SMEM_tx.eps}
      \vspace{-7mm}
      \caption{Data transfer with Tx from IO SMEM to CC.}
      \label{fig:IO_SMEM_tx}
    \end{minipage}
    &
    % \setcounter{figure}{7}
    \begin{minipage}[t]{0.24\textwidth}
      \includegraphics[width=1.0\linewidth]{../../R/mppa/graph/eps/BoxPlot_IO_SMEM_uc.eps}
      \vspace{-7mm}
      \caption{Data transfer with UC from IO SMEM to CC.}
      \label{fig:IO_SMEM_uc}
    \end{minipage}
    \vspace{-3mm}
  \end{tabular}
  \vspace{-3mm}
\end{figure*}

In the Tx interface, DDR causes a large increase in latency.
The time taken by the DDR is twice as that of the IO SMEM as shown in Figure \ref{fig:tx_uc_log}.
This is due to the memory access speed characteristics of DRAM and SRAM.
In the Tx interface, it is necessary for an IO core on IOS to operate the DMA in the IOS NoC interface.
This is attributed to the fact that the core is involved in processing.
The speed of the data transfer latency between CCs exceeds that between IOS and CC.
This result indicates that the MPPA-256 is optimized for communication between the CCs.

With respect to the UC interface, the latency is not significantly affected by the location at which the transferred buffer is allocated (i.e., the DDR or SMEM).
% Similar latency characteristics are observed in Figure \ref{fig:tx_uc_log}.
In the case of the UC interface, an IO core on the IOS does not involve a DMA transaction.
A UC in the NoC interface executes a programed thread sending data.
This evaluation result suggests that the slow access speed of the DDR is not significant in the case of the UC.
In a manner similar to the Tx interface, the speed of the data transfer latency between CCs exceeds that between IOS and CC.

% \begin{figure*}[t]
%   \tabcolsep = 0.5mm              % side-margin in column
%   \begin{tabular}{cc}
%     % \setcounter{figure}{9}
%     \begin{minipage}[t]{0.49\textwidth}
%       \includegraphics[width=1.0\linewidth]{../../R/mppa/graph/eps/BoxPlot_Tx_vs_UC_log.eps}
%       \vspace{-7mm}
%       \caption{Data transfer with Tx/UC (logarithmic axis).}
%       \label{fig:tx_uc_log}
%     \end{minipage}
%     &
%     % \setcounter{figure}{11}
%     \begin{minipage}[t]{0.49\textwidth}
%       \includegraphics[width=1.0\linewidth]{../../R/mppa/graph/eps/BoxPlot_Tx_vs_UC.eps}
%       \vspace{-7mm}
%       \caption{Data transfer with Tx/UC (linear axis).}
%       \label{fig:tx_uc}
%     \end{minipage}
%     \vspace{-3mm}
%   \end{tabular}
%   \vspace{+2mm}
% \end{figure*}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{../../R/mppa/graph/eps/BoxPlot_Tx_vs_UC_log.eps}
  \vspace{-2mm}
  \caption{\label{fig:tx_uc_log}
    Data transfer with Tx/UC }
  \vspace{-4mm}
\end{figure}

% \vspace{-3mm}
% \subsection{Matrix Calculation}
% \label{sec:martix_eval}

% \vspace{-3mm}
% \subsubsection{Situations and Assumptions}
% \label{sec:situations_and_assumptions}
% In the evaluation, matrix calculation time and parallelization potential of MPPA-256 are clarified.
% Matrix calculations are conducted in IOS and CC.
% Three computing situations are considered as shown in Figure \ref{fig:mat_calc}.
% The first situation involves computing in IOS where four cores are available.
% In order to analyze memory access characteristics, a matrix buffer is allocated in IO DDR and SMEM.
% The second situation involves computing in CC in which 16 cores are available.
% The third situation involves offload-computing by using an IOS and four CCs.
% Parallelized processing is executed with four CCs' cores and SMEMs.
% A few cores in IOS and CC manage the parallelized transaction.
% The method can handle large data in which one cluster is not sufficient because buffer capacity is not limited to 2 MB in SMEM.
% % Parallelized processing and the total capacity of SMEM are superior to IOS or CC computations. 
% With respect to the IOS, the application can handle large capacity data only in the DDR.
% However, in this method, distributed memories are used to deal with large capacity data in SMEM.
% Thus, it is necessary for IOS and CC cores to access matrix buffers without cache to avoid cache coherency trouble.
% In order to facilitate faster data transfer, a part of the matrix buffer is transmitted in parallel as shown in Figure \ref{fig:mat_calc}.

% Matrix calculation time is analyzed with parallelization and memory allocation.
% Additionally, the influences of cache is analyzed because cache coherency is an important issue in many-core systems.
% There are several cases in which applications must access specific memory space without a cache.
% With respect to the given assumptions, maximum total buffer size is 1 MB, and thus three matrices buffers are prepared, and each size is 314 KB.
% Matrix A and matrix B are multiplied, and the result is stored in matrix C.
% % The total of the three matrices is set as approximately 1 MB.
% We assume that the remainder of SMEM (1 MB) is occupied with system software and applications in CC.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.8\linewidth]{../figure/matrix_calculation.eps}
%   \vspace{-4mm}
%   \caption{\label{fig:mat_calc}
%     Matrix calculation situations.}
%   \vspace{-5mm}
% \end{figure}

% \vspace{-3mm}
% \subsubsection{Influences of Cache and Memory Type}
% \label{sec:cache_and_memory}
% First, matrix calculation time with the cache in IOS and CC is depicted in Figure \ref{fig:mat_calc_cache}.
% There are almost no differences between IO DDR, IO SMEM, and CC SMEM due to the cache.
% 128 KB data cache in the IOS works well and compensates for the DDR delay.
% Additionally, it is observed that calculation time scales exhibit a linear relation with the number of threads.
% This corresponds to ideal behavior with respect to parallelization.

% Second, matrix calculation time without a cache in the IOS and CC is shown in Figure \ref{fig:mat_calc_uncache}.
% The absence of a cache, results in a fourfold increase in the DDR and a high difference arises with respect to the SMEM.
% Another notable result is that calculation speed in CC SMEM exceeds that of the IO SMEM.
% This characteristic is hidden in the calculation with the cache.
% The computing cores physically involvethe same cores in IOS and CC, and thus it is considered that the characteristics and physical arrangement of SMEM exert a significant effect.
% This is an interesting result since a high difference that cannot be ignored exists.
% It is also observed that calculation time exhibits a linear relation with the number of threads.
% Furthermore, the calculation speed without the cache in CC SMEM exceeds that with cache.
% This result is contrary to intuition, and a cache line problem are conceivable.
% When a small data cache (8 KB) in a PE of CC does not function adequately and an application always misses cache, memory access will pay the time for a non-cached data access and the cost to refill the cache line.
% As a result, the memory access speed without the cache exceeds that with cache.

% \begin{figure*}[t]
%   \tabcolsep = 0.5mm              % side-margin in column
%   \begin{tabular}{cc}
%     \begin{minipage}[t]{0.49\textwidth}
%     \includegraphics[width=1.0\linewidth]{../figure/BarGraph_matrix_with_cache.eps}
%       \vspace{-7mm}
%       \caption{Matrix calculations in IOS and CC with cache.}
%       \label{fig:mat_calc_cache}
%     \end{minipage}   
%     &
%     % \setcounter{figure}{11}
%     \begin{minipage}[t]{0.49\textwidth}
%       \includegraphics[width=1.0\linewidth]{../figure/BarGraph_matrix_without_cache.eps}
%       \vspace{-7mm}
%       \caption{Matrix calculations in IOS and CC without cache.}
%       \label{fig:mat_calc_uncache}
%     \end{minipage}
%     \vspace{-3mm}
%   \end{tabular}
%   \vspace{-2mm}
% \end{figure*}

% \vspace{-3mm}
% \subsubsection{Four CCs Parallelization}
% \label{sec:four_CCs}
% Finally, matrix calculation with offload-computing in IOS and CCs is shown in Figure \ref{fig:mat_calc_offload_314}.
% In this case, it is assumed with respect to the calculation of large matrices that the total capacity exceeds 1 MB.
% The offloading result is compared with IO DDR (cached) due to the fore-mentioned assumption.
% The aggregate calculation is obtained by offloading on the four CC to perform a multiplication of a tile of matA and a tile of the transpose of matB.
% This produces an overhead irrespective of the number of threads as shown in Figures \ref{fig:mat_calc_offload_314} and \ref{fig:mat_calc_offload_640}.
% However, the speed involved in offloading result exceeds that of IO DDR (cached).
% The result indicates several important facts.
% First, D-NoC data transfer produces little overhead latency.
% Second, the speed of DMA memory access to DDR exceeds that of IO core's memory access even if target memory is allocated on DDR.
% In the offloading case, a DMA accesses matrix buffers on DDR and transfers the buffers from IO DDR to each CC SMEM.
% Subsequently, PEs in the CC access matrix buffer the calculation without cache.
% The overhead of data transfer and DMA memory access is small, and thus parallel data transmission and distributed memory are practical in the case of MPPA-256.
% The impact of offloading increases when the matrix size is large as shown in Figure \ref{fig:mat_calc_offload_640}.
% % Only a part of matrix is allocated in CC, and thus it is possible to handle larger matrices buffers.
% % Additionally, 640 KB matrices are prepared, and matrix calculation calculations are evaluated with offload-computing.
% % The speed of the offloading result exceeds that of IO DDR result with respect to the 314 KB matrices in Figure \ref{fig:mat_calc_offload_314}.
% In these offloading evaluations, each CC concurrently transmits calculation results to the IOS.
% \comment{1-22}
% When matrix C in which calculation results are stored is allocated in DDR, NoC router's FIFOs sometimes overflow and cause an error due to memory access delay of DDR.
% This error should be prevented by transmission protocol, and flow control is future work for MPPA-256.
% Currently, to avoid this error, matrix C should be allocated in IO SMEM.
% Note that above evaluation results when matrix C is allocated in DDR.


% \begin{figure*}[t]
%   \tabcolsep = 0.5mm              % side-margin in column
%   \begin{tabular}{cc}
%     \begin{minipage}[t]{0.49\textwidth}
%       \includegraphics[width=0.9\linewidth]{../figure/BarGraph_matrix_with_CCs_314.eps}
%       \vspace{-4mm}
%       \caption{Matrix calculations with offload-computing (314 KB matrix x 3).}
%       \label{fig:mat_calc_offload_314}
%     \end{minipage}   
%     &
%     % \setcounter{figure}{11}
%     \begin{minipage}[t]{0.49\textwidth}
%       \includegraphics[width=0.9\linewidth]{../figure/BarGraph_matrix_with_CCs_640.eps}
%       \vspace{-4mm}
%       \caption{Matrix calculations with offload-computing (640 KB matrix x 3).}
%       \label{fig:mat_calc_offload_640}
%     \end{minipage}
%     \vspace{-3mm}
%   \end{tabular}
%   \vspace{-2mm}
% \end{figure*}

\vspace{-3mm}
\subsection{Practical Application}
\label{sec:practical_application}
\vspace{-3mm}
This wotk adopts a part of an autonomous driving system and this section demonstrates the parallelization potential of the MPPA-256.
We took an algorithm for vehicle self-localization written in C++ in Autoware, namely an open-source software for urban autonomous driving \cite{autoware}, \cite{kato2015open}, and parallelized part of it.
The self-localization adopts the normal-distribution transform matching algorithm \cite{magnusson2009three} implemented in the Point Cloud Library.

% The self-localization algorithm is primarily divided into two processes: \emph{RadiusSearch} which searches for several nearest neighbor points for each query and calculates the distance, and \emph{Derivatives} which calculates the derivative to determine the convergence of the matching operation.
% As shown in Figure \ref{fig:ndt_matching_situation}, 
% This evaluation parallelized \emph{Derivatives} onto 16 CCs and the remainder of the algorithm was executed on the IOS with its four cores.
% To parallelize \emph{RadiusSearch}, the algorithm of the nearest neighbor search needs to be redesigned because the data to be searched exceeds 1 MB.
% Redesigning this algorithm will be part of a future work.

The self-localization algorithm is primarily composed of \emph{computeTransform} function which searches for several nearest neighbor points for each scan query and calculates matching transformation.
This evaluation parallelized a part of \emph{computeTransform} onto 16 CCs and the remainder of the algorithm was parallelly executed on the IOS with its four cores.
To parallelize remainder of \emph{computeTransform} in CCs, the algorithm of the nearest neighbor search needs to be redesigned because the data to be searched exceeds 1 MB.
Redesigning this algorithm will be part of a future work and there are room for improvement by the parallelization potential of the MPPA-256.

As shown in Figure \ref{fig:ndt_matching}, the evaluation of parallelized self-localization algorithm indicates the average execution time for each convergence and demonstrates that the parallelization accelerates the \emph{computeTransform} process.
% Because the reduction of the execution time of \emph{computeTransform} involves reducing the number of loops for convergence, the execution times of \emph{RadiusSearch} and other parts are also shortened.
The query can be assumed to be 10 Hz in many automated driving systems.
Thus, this tuning successfully achieves the deadline.
This parallelized algorithm is executed on simulation and real car experiments in our test course and worked successfully.

% \begin{figure*}[t]
%   \tabcolsep = 0.5mm              % side-margin in column
%   \begin{tabular}{cc}
%     \begin{minipage}[t]{0.49\textwidth}
%       \centering
%       \includegraphics[width=0.7\linewidth]{../figure/ndt_matching.eps}
%       \caption{\label{fig:ndt_matching_situation}
%       A situation of vehicle self-localization execution.}
%     \end{minipage}   
%     &
%     % \setcounter{figure}{11}
%     \begin{minipage}[t]{0.49\textwidth}
%       \includegraphics[width=1.0\linewidth]{../figure/BarGraph_ndt_matching.eps}
%       \caption{\label{fig:ndt_matching}
%       Vehicle self-localization of the partially parallelized autonomous driving application.}
%     \end{minipage}
%     \vspace{-3mm}
%   \end{tabular}
%   \vspace{-2mm}
% \end{figure*}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{../figure/BarGraph_ndt_matching.eps}
  \vspace{-3mm}
  \caption{\label{fig:ndt_matching}
  Vehicle self-localization of the partially parallelized autonomous driving application.}
  \vspace{-5mm}
\end{figure}

% \vspace{-3mm}
% \subsection{Lessons Learned}
% \label{sec:lessons}
% \vspace{-3mm}
% \comment{TBD: 3-4}
% So far, we have quantitatively clarified characteristic of data transfer and parallel computing on NoC-based embedded many cores in Section \ref{sec:evaluations}.
% We can get insight and guidelines for users and developers of NoC-based embedded many cores through MPPA-256.

% From evaluations of D-NoC data transfer, we can get two lessons: influences of NoC routing and DMA.
% First, data transfer latencies between Clusters are hardly influenced by routing for users as shown in Figures \ref{fig:DDR_tx}, \ref{fig:DDR_uc}, \ref{fig:IO_SMEM_tx}, and \ref{fig:IO_SMEM_uc}.
% Software transactions of transmitting and receiving in routers and RM are dominant factors for latencies.
% Second, in the IOS, the latency of UC is not significantly affected by the memory location, the DDR or SMEM, at which the transferred buffer is allocated from the evaluation of Figure \ref{fig:tx_uc_log}.
% The merit of UC is profitable for users because it is common situations to transfer data from DDR to CC SMEM using UC, especially for large data.

% From micro benchmark of matrix calculation, we can get three lessons of memory access: characteristic of SMEM, influences of cache, and data flow control on D-NoC.
% First, memory access speed of CC SMEM exceeds that of the IO SMEM as shown in Figure \ref{fig:mat_calc_uncache}.
% There is a significant difference, which is a major reason to actively handle in CC.
% Second, we quantitatively indicate that the calculation speed without the cache in CC SMEM in Figure \ref{fig:mat_calc_uncache} exceeds that with cache in Figure \ref{fig:mat_calc_cache}.
% It is generally known that cache overhead exists when the miss hit frequently occurs, but it is notable that there is a big difference which can not be ignored by the influences of cache line.
% Third, when data is parallelly transferred from multiple CCs to IO DDR, NoC routers' FIFO sometimes overflows due to memory access delay of DDR.
% This should be prevented by transmission protocol and flow control.

% From the practical application, since we parallelize the vehicle self-localization algorithm of the autonomous driving system, it can be practically used.
% We apply for parallel data transferfrom IOS to CCs and parallel computing on IOS and CC by using CC SMEM as scratch pad memory.

% \vspace{-3mm}
% \section{Related Work}
% \label{sec:related_work}
% \vspace{-3mm}
% This section compares many-core platforms and discusses previous work related to multi/many cores.
% First, comparison of many-core platforms to other platforms is discussed.
% Second, the Kalray MPPA-256 which this work focuses on is compared to other COTS COTS multi/many-core components, and we summarize the features of MPPA-256.
% Finally, discussions of previous work and comparison to them are described.

% % Recently, studies indicate that the single core processors are characterized by limited computation performance.
% % Pollack stated that a single core is inefficient \cite{pollack1999new} and that Moore's law \cite{moore2006cramming} is no longer applicable.
% % Therefore, extant CPUs are not sufficient to satisfy increasing computation demands.
% % The shift from a single core in real-Time and embedded systems has occurred \cite{saidi2015shift}. 
% % Many other platforms including many-core are developed and researched by current studies.

% Table \ref{tb:comparison_platforms} summarizes the features of many-core platforms with those of other platforms.
% For instance, the GPU is a powerful device to enhance computing performance and it has great potential in specific areas (for e.g., image processing, and learning).
% \comment{1-24}
% However, it is mainly used for a specific purpose and its predictability is not suitable for real-time systems.
% It is difficult to use a GPU for many kinds of applications and to guarantee its reliability due to the GPU architecture.
% Many-core processors based on CPU are significantly superior to GPU with respect to software programmability and timing predictability.
% \comment{1-25}
% Additionally, it is commonly known that many-core platforms such as MPPA-256 involve a reasonable power consumption \cite{kanter2015kalray}.
% In contrast, the GPU consumes a significant amount of power and generates considerable heat.
% This is a critical problem for embedded systems.
% FPGAs are also high-performance devices when compared to CPUs.
% They are efficient in terms of power consumption.
% FPGAs guarantee predictability and efficient processing.
% % They are suitable for time-critical computing \cite{de2015kalray}.
% \comment{1-26}
% % However, DSPs cannot be used for many kinds of applications and programming.
% However, FPGAs are difficult for software developers and are not a substitute for CPU since their software model is significantly different from that of CPU.
% Many-core platforms can potentially replace single/multi core CPU as they possess ease of programming and scalability.

% Based on the fore-mentioned background, many COTS multi/many-core components are developed and released by several vendors.
% (e.g., Kalray's the Multi-Purpose Processing Array (MPPA) 256, \cite{de2014time}, Tilera's Tile-Gx \cite{ramey2011tile}, \cite{schooler2010tile}, Tilera's Tile64 \cite{bell2008tile64}, and Intel's Xeon Phi \cite{chrysos2014intel}, \cite{chrysos2012intel}, Intel's Single-chip Cloud Computer (SCC) \cite{baron2010single}).
% The present work focuses on the Kalray MPPA-256 that is designed for real-time embedded applications.
% Kalray, \cite{de2014time} presented clustered many-core architectures on the NoC which pack 256 general-purpose cores with high energy efficiency.

% % \renewcommand{\arraystretch}{1.2}
% \begin{table}[t]
%   \caption{\label{tb:comparison_platforms}
%     Comparison of Many-core to CPU, GPU, and FPGA}
%   \vspace{-3mm}
%   \centering
%   \scriptsize	                    % text size
%   \tabcolsep = 0.4mm              % side-margin in column
%   \begin{tabular}{c|cccccccccc}
%     \hline
%     & \multirow{2}{*}{performance} & \multirow{2}{*}{power/heat} & \multirow{2}{*}{predictability} & \multirow{2}{*}{real-time} & software & \multirow{2}{*}{costs} & multiple\\
%     &&&&& development && instruction \\
%     \hline
%     \hline
%     CPU & & L & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & L \\
%     GPU & \(\checkmark\) &  & L &  & L & \(\checkmark\)\\
%     % DSP & L & L & \(\checkmark\) & \(\checkmark\) & L & \(\checkmark\) & \\
%     FPGA & \(\checkmark\) & L & \(\checkmark\) & L &  & L & \\
%     Many-core & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & L & \(\checkmark\) \\
%     \hline
%   \end{tabular}
%   \begin{flushright}
%      \comment{1-23} \textasteriskcentered In a table, ``L'' means ``limited''.
%   \end{flushright}
%   \vspace{-5mm}
% \end{table}

% % \renewcommand{\arraystretch}{1.2}
% \begin{table}[t]
%   \caption{\label{tb:comparison_manycore}
%     Comparison of Many-core Processor Platforms}
%   \vspace{-3mm}
%   \centering
%   \scriptsize	                    % text size
%   \tabcolsep = 1.5mm              % side-margin in column
%   \begin{tabular}{c|cccc}
%     \hline
%     & \multirow{2}{*}{scalability} & power  & code & \\
%     & & efficiency & transplant & \\
%     \hline
%     \hline
%     Kalray MPPA-256 \cite{de2014time} & \(\checkmark\) & \(\checkmark\) & L & \\
%     Tilera Tile series \cite{bell2008tile64} &  & L & \(\checkmark\) & \\
%     Intel Xeon Phi \cite{chrysos2014intel} \cite{chrysos2012intel} &  &  & \(\checkmark\) & \\
%     Intel SCC \cite{baron2010single} &  &  & \(\checkmark\) & \\
%     \hline
%   \end{tabular}
%   \vspace{-5mm}
% \end{table}

% \comment{1-27}
% MPPA-256 is superior to other COTS multi/many-core components in terms of the scalability of the number of cores and the power efficiency as shown in Table \ref{tb:comparison_manycore}.
% \comment{1-28}
% In terms of the scalability, MPPA-256 adopts 256 cores while other COTS multi/many-core components have 64 cores or the number of cores around it.
% This scalability of cores is attributed to the NUMA memory architecture; each cluster of 16 cores contains its own local shared memory.
% The precise hardware model is described in Section \ref{sec:hardware_model}.
% When all cores share the global DDR memory as in other platforms excluding MPPA-256, specific bus routes receives extremely large loads and memory access contention frequently occurs.
% Local shared memory reduces the above problems and helps the scalability of the number of cores.
% This is why MPPA-256 has succeeded in scaling up to 256 cores and what the ``scalability'' column of Table \ref{tb:comparison_manycore} means.
% However, the NUMA memory architecture restricts the capacity of the memory and requires a data copy from DDR with NoC.
% This restriction makes the use of existing applications difficult especially in the case of applications that require more memory.
% As a result, due to  NUMA memory architecture, portability of code porting to MPPA-256 is inferior to other COTS platforms as shown in Table \ref{tb:comparison_manycore}.

% In terms of power efficiency, MPPA-256 realizes superior energy efficiency despite its large number of cores \cite{kanter2015kalray}.
% The total clock frequency per watt is the highest of the current COTS multi/many-core components.
% The power consumption of the MPPA processor ranges between 16W at 600 MHz and 24W at 800 MHz.
% % While MPPA-256 and TILE64 \cite{bell2008tile64} target the embedded systems, other COTS components consider the HPC accelerator.
% % Their clock frequency per core exceeds that of embedded solutions.
% We need to distinguish the COTS multi/many-core components according to their requirements with reference to Table \ref{tb:comparison_manycore}.
% MPPA-256 is typically accepted with respect to many-core platforms and the model has been used in previous work \cite{becker2016contention}, \cite{carle2014static}, \cite{perret2016mapping}, \cite{perret2016predictable}.

% Previous work have examined real-time applications on many-core platforms including MPPA-256.
% In Ref. \cite{saidi2015shift}, multiple opportunities and challenges of multi/many-core platforms are discussed.
% The shift to multi/many cores in real-time and embedded systems is also described. 

% Based on the above background, several task mapping algorithms for multi/many-core systems have been proposed \cite{carle2014static}, \cite{faragardi2014communication}, \cite{perret2016mapping}.
% Airbus \cite{perret2016mapping} proposes a method of directed acyclic graph (DAG) scheduling for hard real-time applications using MPPA-256.
% In Ref. \cite{faragardi2014communication}, a mapping framework is proposed on the basis of AUTOSAR which is applied as a standard architecture to develop automotive embedded software systems \cite{furst2009autosar}.
% AUTOSAR task scheduling considering contention in shared resources is presented in Ref. \cite{becker2016contention}.

% By examining the above mapping algorithms of real-time applications, previous work \cite{deDinechin2014GSN}, \cite{denet2017work}, \cite{kanter2015kalray}, \cite{perret2016predictable} have analyzed the potential of MPPA-256 and data transfer with NoC as shown in Table \ref{tb:comparison_relatedwork}.
% MPPA-256 is introduced, and its performance and energy consumption are reported in Ref. \cite{kanter2015kalray}.
% However, this report contains few evaluations and does not refer to data transfer with NoC and memory access characteristics.
% Data transfer with NoC in MPPA-256 is described, and NoC guaranteed services are analyzed in Refs. \cite{deDinechin2014GSN} and \cite{denet2017work}.
% While the theoretical analysis is thorough in these work, the practical evaluations are poor and parallel data transfer is not referred to.
% The authors of Ref. \cite{perret2016predictable} focused on the predictable composition of memory accesses.
% An analysis of their work identified the external DDR and the NoC as main bottlenecks for both the average performance and the predictability on platforms such as MPPA-256.
% Although the analysis examined the memory access characteristics of the external DDR and provided notable lessons, a solution for the DDR bottleneck was not examined and practical evaluations were lacking.

% % \renewcommand{\arraystretch}{1.2}
% \begin{table*}[t]
%   \caption{\label{tb:comparison_relatedwork}
%     Comparison of Related Work}
%   \vspace{-3mm}
%   \centering
%   \scriptsize	                    % text size
%   \tabcolsep = 1.5mm              % side-margin in column
%   \begin{tabular}{c|ccccccccc}
%     \hline
%     & performance & data transfer & memory access & \multirow{2}{*}{real applications} & parallel data & \\
%     & analysis & analysis with NoC & characteristics & & transfer & \\
%     \hline
%     \hline
%     Kalray clusters calculate quickly \cite{kanter2015kalray} & L &  &  &  &  & \\
%     Network-on-Chip Service Guarantees \cite{denet2017work} &  & \(\checkmark\) &  &  &  & \\
%     Predictable composition of memory accesses \cite{perret2016predictable} &  & \(\checkmark\) & \(\checkmark\) &  &  & \\
%     this paper & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \\
%     \hline
%   \end{tabular}
%   \vspace{-5mm}
% \end{table*}

\vspace{-3mm}
\section{Conclusions}
\label{sec:conclusion}
\vspace{-3mm}
% This paper has conducted quantitative evaluations that examined end-to-end latencies with embedded many-cores based on NoC, the scalability of parallelization, and the characteristics of the memory access speed.
% From various experiments, we have evaluated the capabilities of the currently available data transfer methods and the parallelization potential for many-core computing based on NoC.
% Our experimental results will allow system designers to choose appropriate system designs based on the requirements of their latency-sensitive many-core applications beyond an intuitive expectation.
This work has conducted quantitative evaluations of data transfer methods on NoCs and practical application with NUMA many cores such as MPPA-256.
Evaluations indicate latency characteristics on NoC, influence of data allocation, and the scalability of parallelization.
Our experimental results will allow system designers to choose appropriate system designs.
At the last, parallelization of real application proofs 
cality of NoC-based embedded NUMA many-cores.

In future work, we will port real-time systems on CC and propose the parallelization of memory intensive algorithms such as the nearest neighbor search in Section \ref{sec:practical_application}.

\vspace{-3mm}
\section*{Acknowledgments}
\vspace{-3mm}
This paper was partly supported by Toyota Motor Company, eSOL, and Kalray.

% 
% for double-blind review
% 
% \section*{Acknowledgements}
% \label{sec:acknowledgements}
% We acknowledge support from eSOL and Kalray.
% The authors would like to thank Atsushi Matsuo and Masaki Gondo from eSOL, and Gilbert Richard, St\'ephane Cordova, and Beno\^\i t Dupont de Dinechin from Kalray for their support that helped improve this study.

\renewcommand{\baselinestretch}{0.8}

\comment{2-5}
\comment{TBD: 3-6}
\vspace{-1mm}
\bibliographystyle{abbrv}       % ACM
\bibliography{reference}

\end{document}

